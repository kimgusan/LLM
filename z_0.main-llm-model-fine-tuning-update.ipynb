{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab44d30b-7aee-47b7-b35e-0e0ecd0a5e21",
   "metadata": {},
   "source": [
    "# 0. gpt 가 아닌 hugging face 의 모델 중 하나를 선택해 llm 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9647c-4055-4f22-8f6b-b13d41db0b47",
   "metadata": {},
   "source": [
    "## 코드 흐름\n",
    "> HuggingFaceHub 모델을 chain 형식으로 사용 후 정상 출력 확인  \n",
    "> 데이터 수집  \n",
    "> 추가 학습을 통한 실제 모델 훈련  \n",
    "> 테스트 진행  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4cf84-a69f-403f-a11e-b3803716989d",
   "metadata": {},
   "source": [
    "### 주제: 전세계의 웹 사이트 주요 기능 설명 챗봇.\n",
    "1. 참고 사이트: semrush\n",
    "- Similarweb: 웹사이트의 트래픽, 사용자 행동, 인기 페이지 등을 분석하여 제공 / 주요 웹사이트의 기능적 장점과 트렌드를 파악.\n",
    "- BuiltWithL 특정 웹사이트에서 사용되는 기술 스택을 분석하여 기능적인 차별점을 파악할 수 있음.\n",
    "- G2: 다양한 소프트웨어와 플랫폼에 대한 리뷰를 제공하며, 웹사이트 기능과 관련된 사용자 평가를 통해 장단점을 분석할 수 있습니다.\n",
    "- Trustpilot: 사용자 리뷰를 통해 웹 사이트의 가용성 및 기능적 강점에 대해 크롤링 할 수 있는 좋은 소스."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02f11f-fe62-4afd-bb21-5b98fc6c3501",
   "metadata": {},
   "source": [
    "### 크롤링 사용 도구:\n",
    "1. BeautifulSoup (Python): HTML 및 XML 문서를 구문 분석하고, 웹사이트의 특정 데이터를 추출하는 데 매우 적합한 크롤링 도구입니다.  \n",
    "   간단한 크롤링 작업에 적합하며, 필요한 정보만 빠르게 수집할 수 있습니다.\n",
    "3. Scrapy: Scrapy는 대규모 크롤링 작업에 적합한 프레임워크로, 빠르게 데이터를 수집하고 정리할 수 있습니다.\n",
    "4. Selenium: 웹 페이지의 동적 요소까지 크롤링해야 할 때 유용합니다. 자바스크립트가 많이 사용된 웹사이트에서 효과적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a81191-d626-4879-a0fe-1724981297d9",
   "metadata": {},
   "source": [
    "### 01. chain형식의 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c658eed-d9cd-4028-877a-b54ce145b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437c8761-5c48-4f1c-8fd3-145bc4262ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "#HuggingFace Repository ID\n",
    "repo_id = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "question = \"Who is Son Heung Min?\"\n",
    "\n",
    "template = \"\"\" Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables = [\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a984d4c-a077-4ad8-b626-2f1f1e0f8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    model_kwargs={\"temperature\":0.2, \"max_length\":128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67bdb6f-a54f-494f-bd01-4bef296f4f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/gl6wpvqx5z596rbpjsq5n3tc0000gn/T/ipykernel_70977/3107618602.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0786dcd-17e8-44a5-ab33-f3ca184c9363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/gl6wpvqx5z596rbpjsq5n3tc0000gn/T/ipykernel_70977/448137714.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  print(llm_chain.run(question=question))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: Who is Son Heung Min?\n",
      "\n",
      "Answer: 27-year-old South Korean forward who plays for Tottenham Hotspur in the Premier League.\n",
      "\n",
      "Question: What is Son Heung Min’s best position?\n",
      "\n",
      "Answer: Left winger.\n",
      "\n",
      "Question: What is Son Heung Min’s best attribute?\n",
      "\n",
      "Answer: His pace.\n",
      "\n",
      "Question: What is Son Heung Min’s best skill?\n",
      "\n",
      "Answer: His dribbling.\n",
      "\n",
      "Question: What\n"
     ]
    }
   ],
   "source": [
    "# 실행 \n",
    "print(llm_chain.run(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30de551-5765-4d8e-a46f-00c04203dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a hammer, cut a small hole in the middle of the hammer and place it on a baking sheet.\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Repository ID\n",
    "# 불러와서 사용하긴 어려운 모델\n",
    "# repo_id = 'google/flan-t5-xxl'\n",
    "# 정확도가 너무 떨어짐\n",
    "repo_id = 'google/flan-t5-small'\n",
    "\n",
    "# 질의내용\n",
    "question = \"how to make hambuger?\"\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=repo_id, \n",
    "#     model_kwargs={\"temperature\": 0.2, \n",
    "#                   \"max_length\": 512}\n",
    "# )\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 실행\n",
    "print(llm_chain.run(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d28e9-eb86-4a29-aca0-8cb1e7d1b3bc",
   "metadata": {},
   "source": [
    "### 01. 모델을 다운받아서 실제 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc960c0-9732-422c-a5e6-c646efcf0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace Model ID\n",
    "# model_id = 'beomi/llama-2-ko-7b'\n",
    "\t\n",
    "model_id = 'Alphacode-AI/AlphaMist7B-slr-v4-slow2'\n",
    "\n",
    "# HuggingFacePipeline 객체 생성\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id, \n",
    "#     device=-1,               # -1: CPU(default), 0번 부터는 CUDA 디바이스 번호 지정시 GPU 사용하여 추론\n",
    "#     task=\"text-generation\", # 텍스트 생성\n",
    "#     model_kwargs={\"temperature\": 0.1, \n",
    "#                   \"max_length\": 64},\n",
    "# )\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"질문: {question}\n",
    "\n",
    "답변: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4ab14-4024-40b9-aee7-cca85e3ebbdf",
   "metadata": {},
   "source": [
    "## 트러블슈팅\n",
    "- 모델 실행 속도가 매우 느린 증상 -> llama-2-ko-7b 모델은 수억에서 수십억게의 매개변수를 가진 데이터 모델\n",
    "- 맥북에서 gpu를 사용하기 위해 torch 와 transformers 라이브러리가 m1 GPU를 사용할 수 있도록 설정 -> device=0 변경  \n",
    "    !pip install torch torchvision torchaudio\n",
    "\n",
    "=> 맥북의 gpu에서는 메모리 부족으로 경고 발생 (메모리 상한선의 제한선을 해제하지 않고 다른 경량화 모델 사용 예정)\n",
    "\n",
    "    # HuggingFacePipeline 객체 재정의\n",
    "    from transformers import pipeline\n",
    "    # llm = HuggingFacePipeline(\n",
    "    #     pipeline=pipeline(\n",
    "    #         \"text-generation\", \n",
    "    #         model=\"/Users/kimkyusan/.cache/huggingface/hub/models--beomi--llama-2-ko-7b/snapshots/4709f8cd6074590ed591e2fdf75499ae76ac4064\", # 이미 로컬에 있는 모델 경로 또는 모델 이름\n",
    "    #         device=0,  # GPU 사용 시 0, CPU 사용 시 -1\n",
    "    #         model_kwargs={\"temperature\": 0.1, \"max_length\": 64}\n",
    "    #     )\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1913f0-a628-484f-884d-f388f9840fd3",
   "metadata": {},
   "source": [
    "## 01. 한국어 다른 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd9ee6-0b66-4456-a957-1377ea232b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754e45933f624e4ba3c267594119a2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a282aab81cd04213a3047233077e1337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf3afd996c475cafa2df9aa7d0e182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cdb61a37c6490f94c168757891b005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb99510f47d94120b66bd41c3225845a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b64afa6503420881161685c5630d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5617212986d34250bc7529d85f0c3100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18d90a1a1454bbba3bb3c6e285f118c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/d1/a5/d1a58c26a4c2b9267a3c8bab6839cf15b1272d12838a0583a99d4cf7e163f8b0/b24ff83b0cf1577fe5be7ff0610502529bad188faced16f1c9e0869f1d32cb49?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00003.safetensors%3B+filename%3D%22model-00001-of-00003.safetensors%22%3B&Expires=1727946439&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzk0NjQzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2QxL2E1L2QxYTU4YzI2YTRjMmI5MjY3YTNjOGJhYjY4MzljZjE1YjEyNzJkMTI4MzhhMDU4M2E5OWQ0Y2Y3ZTE2M2Y4YjAvYjI0ZmY4M2IwY2YxNTc3ZmU1YmU3ZmYwNjEwNTAyNTI5YmFkMTg4ZmFjZWQxNmYxYzllMDg2OWYxZDMyY2I0OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PkdEf5FnPxsGu4agn7dQj9oyZQ8dL%7ELXwO0x3cJdJRswPSNjbebrhqN98srS6RGQFzkS9hfZcHXRFgboMhZ5b70GO39RlaEdMOyuOKGzPtZE8F06BP4PgZ2f1Ffg-XBR-qEnS%7EIccsOASEtcPcSTUwgJ7Y-IMSdNBfrOMdk%7E4lkHrvpB-t1anqmt1U8m%7EDblR403EcdB9k-xlney5ZzYayzbAcfHjOjESmQ2FqQgUSvDmmTOMWceKMK1U8HVUPzgWKBSVbjCGRScHVFuX-lmq%7EdSMrVGcW1Sa9ZIDTBUS%7E7BGv-1gKpSF2FPdgaUPUEsD-2EAcC5hXyZeIiA57FIAg__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6fc430ac0f427196ba7ae87e6a6fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:  92%|#########1| 4.54G/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5e31c20e1e43fd9eea98c87f6f22aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b350b0c3a8e494ca6ff380edbf028c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace Model ID\n",
    "\t\n",
    "model_id = 'Alphacode-AI/AlphaMist7B-slr-v4-slow2'\n",
    "\n",
    "# HuggingFacePipeline 객체 생성\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id, \n",
    "    device=0,               # -1: CPU(default), 0번 부터는 CUDA 디바이스 번호 지정시 GPU 사용하여 추론\n",
    "    task=\"text-generation\", # 텍스트 생성\n",
    "    model_kwargs={\"temperature\": 0.1, \n",
    "                  \"max_length\": 64},\n",
    ")\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"질문: {question}\n",
    "\n",
    "답변: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab6e9e-2968-46f1-8d04-39907405ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "question = \"대한민국의 수도는 어디야?\"\n",
    "print(llm_chain.run(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071c972-64fd-4e9e-9ff6-de41592e71e6",
   "metadata": {},
   "source": [
    "## 02. 데이터 추출\n",
    "02-1. BeautifulSoup을 사용하여 전세계 웹사이트 확인 (semrush)  \n",
    "02-2. csv 파일로 변경 -> 사이트 제작 나라 feature 추가\n",
    "02-3. 각 사이트에 대한 주요 기능 feature 추가   \n",
    "-> 다양한 검색에 대한 내용이 필요하므로 scrapy, selenium 을 사용할 수 있는 지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8072690-06e6-4bd1-a6b4-37fda83b9a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
