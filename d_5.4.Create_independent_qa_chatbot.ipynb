{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab44d30b-7aee-47b7-b35e-0e0ecd0a5e21",
   "metadata": {},
   "source": [
    "# 5.4 독립형 질문 챗봇 만들기\n",
    "- 라이브러리: langchain, streamlit, PyPDF2\n",
    "- 언어모델: gpt-3.5-turbo\n",
    "- 임베딩모델: all_MiniLM-L6-v2\n",
    "- 벡터 데이터베이스: 파이스(FAISS)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9647c-4055-4f22-8f6b-b13d41db0b47",
   "metadata": {},
   "source": [
    "## 코드 흐름\n",
    "> 내가 가지고 있는 PDF 파일을 이용하여 RAG를 구성  \n",
    "> (질문을 받으면 먼저 내가 가지고 있는 PDF파일에서 내용을 찾아보고 결과를 llm에 제공하여 답변을 하는 방식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716faa8b-ba6b-4f4e-a989-2e2a9199bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846ab65e-6668-4a40-9f0a-94eed460b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 문서에서 텍스트를 추출\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 지정된 조건에 따라 주어진 텍스트를 더 작은 덩어리로 분할\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # separator는 청크 분할 기준, chunk_size는 청크 크기, chunk_overlap은 청크간 중복 크기\n",
    "        separators = \"\\\\n\",\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len\n",
    "    )\n",
    "    chunk = text_splitter.split_text(text)\n",
    "    return chunk\n",
    "\n",
    "# 주어진 텍스트 청크에 대한 임베딩을 생성하고 파이스(FAISS)를 사용하여 벡터 저장소를 생성.\n",
    "def get_vectorstore(text_chunks):\n",
    "    # 임베딩 처리(벡터 변환), HuggingFaceEmbeddings 모델 사용\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ad26f6-906d-4ba2-ad4e-7551c1e650d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 벡터 저장소로 대화 체인을 초기화\n",
    "def get_conversation_chain(vectorstore):\n",
    "    # ConversationBufferWindowsMemory에 이전 대화 저장\n",
    "    # ConversationBufferWindowMemory를 사용하여 이전 대화의 기록을 관리합니다.\n",
    "    # memory_key는 메모리 내에서 대화 내용을 저장하는 키 이름입니다.\n",
    "    # return_message는 대화 내용을 반환할지 여부를 설정합니다.\n",
    "    memory = ConversationBufferWindowMemory(memory_key='chat_history', return_message=True)\n",
    "    \n",
    "    # ConversationRetrievalChain을 통해 랭체인 쳇봇에 쿼리 전송\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm = ChatOpenAI(temperature=0, model_name = 'gpt-3.5-turbo'),\n",
    "        # retriever는 벡터 저장소를 검색 엔진으로 사용하여 쿼리와 가장 관련있는 내용을 검색합니다.\n",
    "        retriever = vectorstore.as_retriever(),\n",
    "        # get_chat_history는 대화의 역사를 관리하는 함수를 지정합니다. 여기서는 전달된 인자를 그대로 반환합니다.\n",
    "        get_chat_history = lambda h: h,\n",
    "        # memory는 위에서 설정한 대화 내용 기록 메모리를 사용합니다.\n",
    "        memory = memory\n",
    "    )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd7848e-6b77-4a4e-9b6a-874e6517dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 12:29:10.240 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.240 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.243 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.338 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/llm_project/lib/python3.8/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-09-20 12:29:10.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:10.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "user_uploads = st.file_uploader(\"파일을 업로드해 주세요~\", accept_multiple_files=True)\n",
    "\n",
    "if user_uploads is not None:\n",
    "    if st.button(\"Upload\"):\n",
    "        with st.spinner(\"차리중..\"):\n",
    "            # PDF 텍스트 가져오기\n",
    "            raw_text = get_pdf_text(user_uploads)\n",
    "            # 텍스트에서 청크 검색\n",
    "            text_chunks = get_text_chunks(raw_text)\n",
    "            # PDF 텍스트 저장을 위해 파이스(FAISS) 벡터 저장소 만들기\n",
    "            vectorstore = get_vectorstore(text_chunks)\n",
    "            # 대화 체인 만들기\n",
    "            st.session_state.conversation = get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c1d6edf-e1a6-4d90-baaf-3fc1ae3d4b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 12:29:16.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:16.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:16.902 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-20 12:29:16.903 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "if user_query := st.chat_input(\"질문을 입력해주세요~\"):\n",
    "    # 대화 체인을 사용하여 사용자의 메시지를 처리\n",
    "    if 'conversation' in st.session_state:\n",
    "        result = st.session_state.conversation({\n",
    "            \"question\": user_query,\n",
    "            \"chat_history\": st.session_state.get('chat_history', [])\n",
    "        })\n",
    "        response = result[\"answer\"]\n",
    "    else:\n",
    "        response = \"먼저 문서를 업로드해주세요~.\"\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.write(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "llm_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
